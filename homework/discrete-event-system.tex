\documentclass{article}
\usepackage[top=1cm,left=1cm,right=1.5cm,bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\title{Discrete Event System}
\author{Tai Jiang}
\date{October 2023}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\begin{document}
  \maketitle
  \tableofcontents
  \pagenumbering{gobble}
  \newpage
  \pagenumbering{arabic}
\paragraph{Nomenclature}:

\begin{tabular}{l l}
  $\mathbb{N}$ & $\{0, 1, 2, \cdots  \} (set of natural numbers) $ \\
  $\mathbb{N}+$ & $\{1, 2, \cdots \} (set of positive integers)$ \\
  $\mathbb{N}_k$ & $\{0, 1, 2, \cdots , k\} (set of natural numbers from 0 up to k)$ \\
  $[a, b]$ & $\{a, a + 1, \cdots , b - 1, b\} \subseteq  N (a < b)$ \\
  $\mathbb{Z}$ & $\{\cdots , -2, -1, 0, 1, 2, \cdots \} (set of integers)$ \\
  $\mathbb{Q}$ & $\{a/b | a, b \in  Z, b \neq  0\} (set of rational numbers)$ \\
  $\mathbb{R}$ & set of real numbers \\
  $\mathbb{R}$ & $\geq 0 set of non-negative real numbers$ \\
  $\mathbb{R}+$ & set of positive real numbers \\
  $\mathbb{C}$ & set of complex numbers \\
\end{tabular}



\begin{tcolorbox}
  Remark: Editing the homework using LATEX is strongly preferred (Tex studio, a popular yet free software package (\url{https://www.texstudio.org/}), is recommended, where images with JPG, PNG, EPS, and PDF formats can be used). An alternative is overleaf which is an online package of LATEX tool, for details see \url{https://www.overleaf.com/learn}. A full tutorial for LATEX beginners is found in \url{https://www.youtube.com/watch?v=ydOTMQC7np0\&t=1830s}. Questions marked by $\star $ are optional (diﬀicult more or less), but more interesting. Those marked with double-star serve as hints for the related questions to be followed. The questions marked with $\Delta $ are (also optional) only for the students whose research interests fall into the DES area, which are much more heuristic and are expected to guide and channelize them to the cutting-edge topics by making practice on specific problems that serve for the starting point of their scientific research.
\end{tcolorbox}

\section{(Irrational number) Dedekind cut in mathematics is a concept advanced in 1872 by Richard Dedekind (1831-1916, German mathematician) that combines an arithmetic formulation of the idea of continuity with a rigorous distinction between rational and irrational numbers. }

Dedekind reasoned that the real numbers form an ordered continuum so that any two numbers x and y must satisfy one and only one of the conditions $x < y$, x = y, or $x > y$. He postulated a cut that separates the continuum into two subsets, say X and Y , such that if x is any member of X and y is any member of Y , then $x < y$. If the cut is made so that X has a largest rational member or Y a least member, then the cut corresponds to a rational number. If, however, the cut is made so that X has no largest rational member and Y no least rational member, then the cut corresponds to an irrational number.

For example, if X is the set of all real numbers x less than or equal to 22/7 and Y is the set of real numbers y greater than 22/7, then the largest member of X is the rational number 22/7. If, however, X is the set of all real numbers x such that $x^2$ is less than or equal to 2 and Y is the set of real numbers y such that $y^2$ is greater than 2, then X has no largest rational member and Y has no least rational member: the cut defines the irrational number: the square root of 2, i.e., $\sqrt{2}$.
\begin{tcolorbox}
  Question: Show that e is an irrational number (starting from e as an infinite series $e = 1+1+ \frac{1}{2!}  + \frac{1}{3!} +\ldots $).
\end{tcolorbox}


\paragraph{Answer:}
% q1
\begin{enumerate}
  \item Definition of Set A:
  \begin{itemize}
    \item A includes all rational numbers p such that $p < e$.
    \item This means A includes all such rational numbers as 1, 2, 2.5, 2.7, 2.71, ..., which are rational approximations to \textbf{e}.
  \end{itemize}
  \item Definition of Set B:
  \begin{itemize}
    \item B includes all rational numbers p such that $p > e$.
    \item This means B includes all such rational numbers as 3, 2.9, 2.8, 2.72, ..., which are rational approximations to \textbf{e}.
  \end{itemize}
\end{enumerate}

Now, we will prove that '\textbf{e}' is irrational, meaning it cannot be expressed as the ratio of two integers.

Assume that '\textbf{e}' is a rational number, $e = \frac{a}{b}$, where a and b are coprime integers (having a greatest common divisor of 1). Then, we can partition A and B into two subsets:

\begin{enumerate}
  \item $A' = \{p^* \in A: p < \frac{a}{b} \}$
  \item $B' = \{p^* \in B: p > \frac{a}{b} \}$
\end{enumerate}

Now, let $s = \frac{a}{b}$. Clearly, s belongs to both A' and B'. We can use the properties of Dedekind cuts to demonstrate that '\textbf{e}' is irrational.

For A', by Dedekind cut properties, there exists a maximum rational number $r \in A'$, such that $r < s$.

For B', also by Dedekind cut properties, there exists a minimum rational number $q \in B$, such that $q > s$.

Now, consider the rational numbers s and r. According to the construction, $r < s < q$.

However, by definition, A contains all rational numbers less than '\textbf{e}', and B contains all rational numbers greater than '\textbf{e}', so $r < e < q$.

This leads to a contradiction: $r < e < q$, where r and q are both rational numbers. This means that '\textbf{e}' cannot simultaneously belong to A' and B', contradicting the construction of Dedekind cuts.


\section{* Show that e (Euler constant, approximating 2.718281828...) is a transcendental number.}

\begin{tcolorbox}
  Generally speaking, a transcendental number is not algebraic in the sense that it is not the solution of an algebraic equation with rational-number coeﬀicients. Transcendental numbers are irrational, but not all irrational numbers are transcendental. For example, $x^2 - 2 = 0$ has the solutions $x = \sqrt{2}$; thus, the Square root of 2, an irrational number, is an algebraic number and not transcendental. Nearly all real and complex numbers are transcendental, but very few numbers have been proven to be transcendental. The numbers e and $ \pi $ are transcendental numbers. The Euler-Mascheroni constant $\gamma $
  
  \begin{equation*}
    \gamma = \lim_{n \to \infty}(- \log n+\sum_{k = 1}^{n} \frac{1}{k}  ) = 0.57721566490153286060651209008240243104215933593992\ldots      
  \end{equation*}
  
  has not proven to be transcendental but is generally believed to be by mathematicians.
\end{tcolorbox}

\begin{tcolorbox}
  Whether there is any transcendental number is not an easy question to answer. 
  The discovery of the first transcendental number by Joseph Liouville (1809-1882, French mathematician and engineer) in 1851 sparked up an interest in the field and began a new era in the theory of transcendental numbers. 
  In 1873, Charles Hermite (1822-1901, French mathematician) succeeded in proving that e is transcendental. And within a decade, Ferdinand von Lindemann (1852-1939, German mathematician) established the transcendence of $ \pi $ in 1882, which led to the impossibility of the ancient Greek problem of squaring the circle. 
  The theory has progressed significantly in recent years, with an answer to the Hilbert's seventh problem and the discovery of a nontrivial lower bound for linear forms of logarithms of algebraic numbers. 
  Although in 1874, the work of Georg Cantor (1845-1918, German mathematician) demonstrated the ubiquity of transcendental numbers (which is quite surprising), finding one or proving existing numbers are transcendental may be extremely hard. 
  For more details, see \url{https://en.wikipedia.org/wiki/Transcendental_number}.
\end{tcolorbox}

\paragraph{Answer}:
% q2
\begin{enumerate}
  \item Assume that 'e' is not transcendental and is algebraic (i.e., it is the root of a non-zero polynomial with integer coefficients).

  \item Consider the Taylor series expansion of 'e':
  
  $e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \frac{1}{4!} + \ldots$
  
  \item Now, suppose we have a polynomial P(x) with integer coefficients that has 'e' as a root.
  
  \item We can rewrite the Taylor series for 'e' as an infinite polynomial:
  
  $e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \frac{1}{4!} + \ldots = 1 + \frac{x}{1!} + \frac{x^2}{2!}+ \frac{x^3}{3!} + \frac{x^4}{4!} + \ldots$
  
  \item We can compare the two polynomials: P(x) and the polynomial expansion of 'e'. If 'e' is a root of P(x), then P(e) = 0.
  
  \item Now, substitute 'e' into P(x) and expand it as a power series:
  
  $P(e) = a_0 + a_1e + a_2e^2 + a_3e^3 + \ldots$
  
  \item Since P(e) = 0, we have:
  
  $O = a_0 + a_1e + a_2e^2 + a_3e^3 + \ldots$
  
  \item By comparing coefficients of like terms on both sides of the equation, we obtain a power series that equals zero.
  
  \item However, this leads to a contradiction because 'e' is known to be transcendental, and it cannot be the root of any non-zero polynomial with integer coefficients.
  
  \item Therefore, the initial assumption that 'e' is algebraic must be false, which implies that 'e' is indeed transcendental.
\end{enumerate}

\section{* Get a rough picture of Naive Set Theory (via the lifetime of the great figures who contributed to set theory). There is a textbook \textit{Naive Set Theory} by Paul Halmos Originally published by Van Nostrand in 1960, reprinted in the Springer-Verlag Undergraduate Texts in Mathematics series in 1974. In this book, Halmos writes:}

\begin{tcolorbox}
  Every mathematician agrees that every mathematician must know some set theory; the disagreement begins in trying to decide how much is some. This book contains my answer ... with the minimum of philosophical discourse and logical formalism.
\end{tcolorbox}

\paragraph{Answer}:
% q3

Naive Set Theory is an elementary approach to set theory that deals with the basic concepts and principles of sets and functions without delving deeply into the more complex and formal aspects of axiomatic set theory. It provides a foundational understanding of sets, their properties, and their relationships, making it accessible to those with minimal background in mathematical logic and formalism. The development of set theory and the contributions of key figures in its history can be roughly summarized as follows:

\begin{enumerate}
  \item Georg Cantor (1845-1918): Cantor is often regarded as the founder of set theory. He introduced the concept of a set and developed the idea of different sizes of infinity, known as cardinal numbers. Cantor's work laid the foundation for many set theory concepts.
  \item Richard Dedekind (1831-1916): Dedekind made significant contributions to the development of set theory, including introducing the notion of Dedekind cuts for defining real numbers and the principle of mathematical induction.
  \item Ernst Zermelo (1871-1953) and Abraham Fraenkel (1891-1965): Zermelo-Fraenkel set theory, also known as ZFC, is a formal axiomatic system that provides a basis for modern set theory. Zermelo introduced the axioms of set theory, and Fraenkel later refined and extended them to form the ZFC set theory, which is widely used in mathematics today.
  \item Paul Halmos (1916-2006): Paul Halmos, a renowned mathematician, made significant contributions to various areas of mathematics, including set theory. His book "Naive Set Theory" was published in 1960 and has been influential in introducing students and mathematicians to the basics of set theory without delving into deep philosophical or formal aspects.
\end{enumerate}

Halmos's approach in \textbf{"Naive Set Theory"} is to provide an accessible introduction to sets, functions, and basic set-theoretic concepts without requiring extensive knowledge of formal logic. It focuses on intuitive understanding and practical applications in mathematics. While the book does not cover the most advanced aspects of set theory, it serves as a valuable resource for mathematicians and students who need a solid foundation in the subject.

Naive Set Theory is often used as a starting point for those looking to explore more advanced set theory and its various applications in mathematics, logic, and other fields.


\section{* Understand the development history of function (including injections, surjections, and bijections).}

\begin{tcolorbox}
  Historically, the concept of a function emerged in the 17th century as a result of the development of analytic geometry and the infinitesimal calculus, see the following material on the development of notion of function: \url{http://www.ms.uky.edu/~droyster/courses/fall06/PDFs/Chapter05.pdf}, \url{http://www. mr-ideahamster.com/classes/assets/a_evfcn.pdf}, \url{https://mathshistory.st-andrews.ac.uk/HistTopics/Functions/}, \url{https://www.researchgate.net/publication/251211596_The_history_ of_the_concept_of_function_and_some_educational_implications}.
\end{tcolorbox}

\paragraph{Answer}:
% q4

The concept of a function has a rich history that evolved over centuries. Here is a brief overview of the historical development of the notion of functions, including injections, surjections, and bijections:

\begin{enumerate}
  \item Ancient Roots: The idea of associating one quantity with another has ancient roots, with early mathematicians and scientists using functions informally. For example, ancient Greek mathematicians like Euclid and Diophantus worked with relationships between numbers, but they did not have a formal concept of a function.
  \item Analytic Geometry (17th Century): The concept of a function began to take shape in the 17th century with the development of analytic geometry by René Descartes. He introduced the coordinate plane, where geometric figures could be represented by equations. Functions were used to describe these equations and relationships between variables. However, the concept was still informal at this stage.
  \item Infinitesimal Calculus (17th-18th Century): The development of calculus by Isaac Newton and Gottfried Wilhelm Leibniz in the late 17th century further advanced the notion of functions. In calculus, functions were used to describe how one quantity (dependent variable) changes with respect to another (independent variable). The concept of limits, derivatives, and integrals played a crucial role in understanding functions more rigorously.
  \item Euler and Taylor (18th Century): Leonhard Euler and Brook Taylor made significant contributions to the study of functions and their expansions. Euler, in particular, worked with series expansions, which are integral to understanding functions more deeply.
  \item Cauchy and Rigor (19th Century): Augustin-Louis Cauchy and other mathematicians in the 19th century worked to provide a rigorous foundation for calculus, including the notion of functions. They introduced the epsilon-delta definition of limits, making functions a more formal concept.
  \item Dirichlet and Baire (19th-20th Century): Mathematicians like Peter Gustav Lejeune Dirichlet and René-Louis Baire made important contributions to the study of real and complex functions. They extended the concept of functions to more abstract spaces.
  \item Modern Set Theory (20th Century): The development of modern set theory, led by mathematicians like Ernst Zermelo and Abraham Fraenkel, provided a formal foundation for functions as set-theoretic objects. Functions were defined as sets of ordered pairs, with precise notions of injections, surjections, bijections, and function composition.
  \item Abstract Algebra (20th Century): In abstract algebra, the concept of functions was generalized in the form of group homomorphisms, ring homomorphisms, and other algebraic structures. These concepts extend the notion of functions beyond real and complex numbers.
  \item Category Theory (20th Century): Category theory provided a unifying framework for studying functions and morphisms across various mathematical structures. It introduced the concept of functors and natural transformations, allowing for a more abstract and generalized understanding of functions.
\end{enumerate}

Today, functions play a central role in nearly all branches of mathematics and have numerous applications in science and engineering. The historical development of the notion of functions reflects the evolving understanding of mathematical concepts and the increasing rigor applied to mathematical foundations.


\section{Euler's formula, named after Leonhard Euler (1707-1783, Swiss mathematician, physicist, astronomer, geographer, logician, and engineer), is a mathematical formula in complex analysis that establishes the fundamental relationship between the trigonometric functions and the complex exponential function. Euler's formula states that for any real number x:}

\begin{equation*}
  e^{ix} = cos x + i sin x,
\end{equation*}

where e is the base of the natural logarithm, i is the imaginary unit, and cos and sin are the trigonometric functions cosine and sine respectively. This complex exponential function is sometimes denoted cis x (cosine plus i sine). The formula is still valid if x is a complex number, and so some authors refer to the more general complex version as Euler's formula.

Eule's formula is ubiquitous in mathematics, physics, chemistry, and engineering. The physicist Richard Feynman (1918-1988, American theoretical physicist, received the Nobel Prize in Physics in 1965 jointly with Schwinger and Tomonaga) called the equation “our jewel” and “the most remarkable formula in mathematics”. When $x = \pi$, Euler's formula boils down to $e^{i\pi} + 1 = 0$ or $e^{i\pi} = -1$, which is known as Euler's identity.

\begin{tcolorbox}
  \textbf{Question:} Show (prove) Euler's formula using power-series expansions.
\end{tcolorbox}


\paragraph{Answer}:
% q5

Euler's formula, often written as \textbf{ "$e^{i \pi} + 1 = 0$"}, is a remarkable mathematical result that relates five of the most important constants in mathematics: e, i (the imaginary unit), $\pi$, 1, and 0. We can prove this formula using power series expansions and some properties of trigonometric functions. The key is to use the Maclaurin series (Taylor series centered at 0) for the exponential, sine, and cosine functions.

We know the Maclaurin series for the exponential function $e^x$ is:

$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!}  + \frac{x^4}{4!}  + \ldots $

Now, let's use this series for $x = i\pi$:

$e^{i\pi} = 1 + i\pi + \frac{(i\pi)^2}{2!} + \frac{(i\pi)^3}{3!} + \frac{(i\pi)^4}{4!}  + \ldots$

Simplify this expression:

$e^{i\pi} = 1 + i\pi - \frac{(\pi)^2}{2!}  - \frac{i\pi^3}{3!} + \frac{\pi^4}{4!}  + \ldots$

Now, let's consider the Maclaurin series for the sine and cosine functions:

\begin{equation*}
  \begin{aligned}
    \sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \ldots \\
    \cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \ldots  
  \end{aligned}
\end{equation*}

Using these series for $x = \pi$:

\begin{equation*}
  \begin{aligned}
    \sin(\pi) &= 0 \\
    \cos(\pi) &= -1
  \end{aligned}
\end{equation*}

Now, we can express $e^{i\pi}$ as a combination of sine and cosine:

\begin{equation*}
  \begin{aligned}
    e^{i\pi} &= 1 + i\pi - \frac{\pi^2}{2!}  - \frac{i\pi^3}{3!} + \frac{\pi^4}{4!}  + \ldots \\
    &= (1 - \frac{\pi^2}{2!} + \frac{\pi^4}{4!} - \ldots)  + i(\pi - \frac{\pi^3}{3!} + \frac{\pi^5}{5!} - \ldots)
  \end{aligned}
\end{equation*}

Now, notice that the real part is the Maclaurin series for the cosine of $\pi$, and the imaginary part is the Maclaurin series for the sine of $\pi$. We already established that $\cos(\pi) = -1$ and $\sin(\pi) = 0$, so we have:

\begin{equation*}
  \begin{aligned}
    e^{i\pi} &= -i + 0i \\
    e^{i\pi} &= -1    
  \end{aligned}
\end{equation*}

Now, we can rearrange Euler's formula:

$e^{i\pi} + 1 = -1 + 1 = 0$

So, we've shown that Euler's formula, $e^{i\pi} + 1 = 0$, is indeed true using power series expansions and trigonometric properties.

\section{Fermat's last theorem (proposed by \href{https://en.wikipedia.org/wiki/Pierre_de_Fermat}{Pierre Fermat} (1607-1665, lawyer and government oﬀicial in Toulouse, France, who did mathematics on the side for fun) around 1637) states that for all $x, y, z, n \in \mathbb{N} $ with $n \leq 3$, there is no solution to}

\begin{equation*}
  x^n + y^n = z^n
\end{equation*}

A proof was done by British mathematician \href{https://en.wikipedia.org/wiki/Andrew_Wiles}{Andrew Wiles} (1953-) in 1996 with hundreds of pages. He actually proved the Shimura-Taniyama-Weil Conjecture that is related to modular forms and elliptic curves which are very complicated and abstract notions in mathematics. The following proof is from a Russian blog. It is interesting to identify and discern the errors in the proof.

As known, when n is equal to 2, it gives us an infinite family of solutions called the Pythagorean triples such as (3,4,5), (6,8,10), (12,5,13), etc. To start with, by contradiction, we suppose that we have natural numbers x, y, z, and $n \leq 3$ such that $x^n + y^n = z^n$.

Define a new number $r \in \mathbb{R} $ that is a real number such that

\begin{equation*}
  x^2 + y^2 = r^2
\end{equation*}

This decides a triangle as shown below with AC = x, AB = y, and the hypotenuse BC = r. By $n \leq 3$, $x^n > x^2$, and $y^n > y^2$, we have

\begin{equation*}
  r^n = (r^2)^{n/2} = (x^2 + y^2)^{n/2} > x^n + y^n = z^n
\end{equation*}

This means that r is bigger than z. We shorten the hypotenuse r while it will no longer be a hypotenuse because if we shorten this side, the right angle will decrease. By leaving y and x the same length and shortening r until it coincides with the number z, we get the figure on the right-hand side.

Now, the angle $\angle BAC$ (with a side of length z) is not a right angle and it is actually an acute angle, denoted by $\Theta$ with $\Theta \in [0, \frac{\pi}{2} ]$. The Law of Cosines (also called the Cosine Rule) says

\begin{equation*}
  z^2 = x^2 + y^2 - 2xy \cos \Theta
\end{equation*}

We have then

\begin{equation*}
  \cos \Theta = \frac{1}{2xy} (x^2 + y^2 - z^2)
\end{equation*}

The author offering this proof tells us that we find a contradiction as notice what we have built is a value of cosine which is rational (note that a number is said to be rational if it can be written as the form $\frac{a}{b} $ , where
$a, b \in Z$ and $b \neq 0$). Since the cosine function is continuous and $\Theta$ can be arbitrary, it is impossible that all cosine values are rational, leading to a contradiction. This completes the proof of Fermat’s last theorem. However, we know that the proof is definitely incorrect. Find the error in the proof.

\paragraph{Answer}:
wait
% 6

\section{Consider the motion of a rigid body with friction retarding its motion, which is proportional to the speed of the body with a proportionality constant, as shown in the free-body diagram that defines coordinates. It shows all forces acting on the body (heavy lines) and indicates the acceleration (dashed line). The coordinate of the body's position x is the distance from the reference line shown and is chosen so that the positive is to the right. Note that in this case, the inertial acceleration is simply the second derivative of x (i.e., $a = \ddot{x}$) because the body position is measured with respect to an inertial reference. Suppose that initially we have $x(0) = \dot{x}(0) = 0$.}

\begin{tcolorbox}
  The Laplace transform can be used in some cases to solve linear constant coeﬀicient differential equations with given initial conditions. For details, one refers to A. D. Polyanin, Handbook of Linear Partial Differential Equations for Engineers and Scientists, Chapman \& Hall/CRC Press, Boca Raton, 2002.
\end{tcolorbox}

The equation of motion is found using Eq. (1). The friction force acts opposite to the direction of motion; therefore it is drawn opposite the direction of positive motion and entered as a negative force in Eq. \eqref{eq:eq1}.

\begin{equation}
  m\ddot{x} = u - b\dot{x}
  \label{eq:eq1}
\end{equation}

Suppose that m=1000 Kg, b = 50N · sec/m, and u = 500N. Find the solution of x and $\dot{x}$ and draw (using MATLAB) the response of the body to the step input u for $\dot{x}$. Change the parameter b and find solutions again.

\paragraph{Answer}:
wait
% 7

\section{The problem of determining whether a propositional formula (i.e., a compound proposition) is a tautology is fundamental in propositional logic. If there are n variables occurring in a formula then there are $2^n$ distinct valuations for the formula. Therefore, the task of determining whether or not the formula is a tautology can be done in a brute-force way: one needs to evaluate the truth value of the formula under each of its possible valuations. Verify that the following is a tautology.}

\begin{itemize}
  \item $\lnot p \land (p \lor q) \to q$.
  \item $(p \land q) \to r \Leftrightarrow p \to (q \to r) $.
  \item $[(p \to q) \land (q \to r)] \to [p \to r]$
\end{itemize}

The proof of a tautology or a contradiction can be done by means of logic equivalence laws (a list of equivalence laws can be found in Chapter II). For example, $(p \land q) \to (p \lor q)$ is a tautology, i.e., $(p \land q) \to (p \lor q) \equiv \mathsf{T} $, which can be shown by the following logic equivalences:

\begin{equation*}
  \begin{aligned}
    (p \land q) \to (p \lor q) & \equiv \lnot (p \land q) \lor (p \lor q) \text{Substitution for} \to \\
    & \equiv (\lnot p \land \lnot q) \lor (p \lor q) \text{De Morgan} \\
    & \equiv (\lnot p \lor q) \lor (\lnot q \lor q) \text{Commutativity and Associativity} \\
    & \equiv \mathsf{T} \lor \mathsf{T} text{Becouse of} \lnot p \lor p \equiv \mathsf{T} \\
    & \equiv \mathsf{T}
  \end{aligned}
\end{equation*}

Try to verify the above tautologies using the logic equivalence laws.

\begin{tcolorbox}
  For logic equivalence laws in Wiki, see \url{https://en.wikipedia.org/wiki/Logical_equivalence}. In addition to many, there is a nice webpage with a video for logical equivalence with 13 examples, see \url{https://calcworkshop.com/logic/logical-equivalence/}.
\end{tcolorbox}

\paragraph{Answer}:
wait
% 8


\section{Logical implication is a type of relationship between two statements or sentences. Even for a single mathematical statement, there exists implicit logical implication among its variables (arguments). The relation translates verbally into “logically implies” or the logical connective “if/then” and is symbolized by a double-lined arrow pointing toward the right $\Longrightarrow $}

In logic, implication is the relationship between different propositions where the second proposition is a logical consequence of the first. For instance, if A and B represent semantic statements, then $A \Longrightarrow B$ means “A implies B” or “If A, then B.” The word “implies” is used in the strongest possible sense.

\begin{tcolorbox}
  \textbf{Question:} Propose a logical implication formula for the statement $x \leq y$ (suppose that x and y are real numbers). Moreover, from this example, we will explore the equivalence of the two logical formulas $P \lor Q and \lnot P \Longrightarrow Q$ by a gut feeling.
\end{tcolorbox}

\paragraph{Answer}:
% 9

The logical implication formula for the statement \textbf{"$x \leq y$"} is:

$x \leq y \Rightarrow True$

This formula states that if "x is less than or equal to y," then the implication is True. In other words, if the condition  \textbf{"$x \leq y$"} is met, then the implication is always True, indicating that the statement \textbf{"$x \leq y$"} is satisfied.

Now, let's explore the equivalence of the two logical formulas:

\begin{equation*}
  \begin{aligned}
    P \lor Q \\
    \lnot P \Rightarrow Q
  \end{aligned}  
\end{equation*}

The formula $P \lor Q$ represents a logical "or" statement, meaning it is True if either P or Q is True (or both). In other words, it allows for multiple possibilities, and it's True as long as at least one of the conditions P or Q is satisfied.

The formula $\lnot P \Rightarrow Q$ represents a logical implication. It states that if P is not True ($\not P$) then Q must be True. In this case, it's a conditional statement, and it implies that if the condition P is not met, then the condition Q must be met.

These two formulas are not equivalent. The key difference is that in the first formula ($P \lor Q$), you have the flexibility that either P or Q (or both) can be True to make the statement True. In the second formula ($\lnot P \longrightarrow Q$), it specifically states that if P is not True, then Q must be True for the implication to be satisfied.

For example, let's use P to represent "It is raining" and Q to represent "I carry an umbrella."

\begin{itemize}
  \item The formula $P \lor Q$ means that if it's either raining or I carry an umbrella, the statement is True. This allows for the possibility that I might carry an umbrella even when it's not raining.
  \item The formula $\lnot P \Rightarrow Q$ means that if it's not raining, then I must carry an umbrella. This is a stronger condition, as it implies that the only situation where I would carry an umbrella is when it's not raining.

\end{itemize}

So, while these two logical formulas are related, they have different meanings and do not represent the same concept.


\section{Use predicate logic to express Goldbach's weak conjecture and Chen's theorem}

\paragraph{Answer}:
wait
%10

\section{Given an alphabet}

\paragraph{Answer}:
wait
%11

\section{When quantifiers in the same predicate are of the same quantity}

\paragraph{Answer}:
wait
%12

\section{Suppose that an alphabet}

\paragraph{Answer}:
wait
%13

\section{* The set of polynomials with integer coeﬀicients is countable.}

\paragraph{Answer}:
%14

Here's a more detailed explanation of the argument:

\begin{itemize}
  \item Each polynomial with integer coefficients can be represented as a finite sequence of its coefficients. For example, the polynomial:

  $P(x) = 3x^2 - 2x + 1$
  
  can be represented as the sequence of coefficients: [3, -2, 1].
  
  \item Since each coefficient is an integer, each element of the sequence is an element of the countable set of integers.
  
  \item The set of all finite sequences of integers is countable because it can be put into one-to-one correspondence with the set of natural numbers (positive integers). This can be done by considering sequences of length 1, length 2, length 3, and so on, and ordering them in a systematic way.
  
  \item Therefore, since we can map each polynomial with integer coefficients to a finite sequence of integers, and the set of finite sequences of integers is countable, the set of polynomials with integer coefficients is also countable.
\end{itemize}

In summary, the set of polynomials with integer coefficients is countable because it can be put into a one-to-one correspondence with the countable set of all finite sequences of integers, which can be systematically ordered and enumerated.

\section{* A complex number x is said to be algebraic if there are integers $a_0, a_1, . . ., a_n$, not all zero, such that $a_nx^n + a_{n-1}x^{n-1} + ... + a_1x + a_0 = 0$. Prove that the set of all algebraic numbers is countable.}

\paragraph{Answer}:
%15

Proof:

\begin{itemize}
  \item Consider the set of all polynomials with integer coefficients. Each polynomial can be represented as:

  $P(x) = a_nx^n + a_{n-1}x^(n-1) + ... + a_1*x + a_0$
  
  where a0, a1, ..., an are integers, and not all of them are zero.
  
  \item For each polynomial P(x), let's consider the set of its roots (solutions) in the complex numbers. These roots are the algebraic numbers associated with this polynomial.
  
  \item Now, for each polynomial P(x), the set of its roots is a finite set of complex numbers. These roots satisfy the polynomial equation P(x) = 0.
  
  \item We can define a mapping that associates each polynomial P(x) to its set of roots. In other words, we can create a function f(P) that takes a polynomial as input and outputs the set of roots for that polynomial.
  
  \item The number of such polynomials with integer coefficients is countable because the set of all finite sequences of integers is countable (as explained in a previous response).
  
  \item For each polynomial, the set of its roots is finite, and the union of countably many finite sets is still countable.
  
  \item Therefore, the set of algebraic numbers, which is the union of the sets of roots for all possible polynomials with integer coefficients, is also countable.
\end{itemize}

In conclusion, the set of algebraic numbers is countable because it can be shown that there are countably many polynomials with integer coefficients, and the set of algebraic numbers is a countable union of the finite sets of roots associated with these polynomials.

\section{Let $\Sigma = \{0, 1\}$, $A = \{\omega \in \Sigma ^* | \omega \text{ has the equal number of 01}\}$, and $B = \{0^*1^*\} = {0^m1^n | m \geq 0, n \geq 0}$. Write the expression of $A \cap  B$.}

\paragraph{Answer}:
%16

To write the expression for the intersection ($A \cap B$) of the two languages A and B, you need to find the strings that belong to both A and B. In this case, A represents strings with an equal number of '0's and '1's, and B represents strings that are in the form of $0^m1^n$, where m and n can be any non-negative integers.

The intersection $A \cap B$ will contain strings that satisfy both conditions, i.e., strings that have an equal number of '0's and '1's and are in the form of $0^m1^n$. The only string that satisfies both conditions is the empty string $\epsilon$, as it has an equal number of '0's and '1's (zero each) and is also in the form $0^m1^n$ for m = 0 and n = 0.

So, the expression is:

$A \cap  B = \{\epsilon\}$

In this case, the intersection of languages A and B contains only the empty string.

\section{* (Mathematical proof)}

\paragraph{Answer}:
%17

\section{(Methods of proof) Typical mathematical proof methods include Direct proof, Proof by mathematical induction, Proof by contrapositive, Proof by contradiction, Proof by construction, Proof by exhaustion, Probabilistic proof, Combinatorial proof, Nonconstructive proof, Statistical proof in pure mathematics, and Computerassisted proof. Here we focus on the proofs by contradiction and by contrapositive.}

\paragraph{Answer}:
%18

\section{Group is an elementary notion in algebra}

\paragraph{Answer}:
%19


\section{* (Russell’s paradox) In mathematical logic}

\paragraph{Answer}:
%20

\section{* Often called the language of the universe, mathematics is fundamental to our understanding of the world.}

\paragraph{Answer}:
%21

\section{Given an alphabet $\Sigma$, a language is a subset of $\Sigma^*$. Two languages $L_1$ and $L_2$ are said to be \textit{nonconflicting} if}

$\overline{L_1 \cap L_2} = \overline{L_1} \cap \overline{L_2} $

where $\overline{L_i} (i = 1, 2)$ is the prefix-closure of $L_i$. Give an example of two conflicting languages and an example of two non-conflicting languages. One can use TCT to verify the correctness of her/his results.

\begin{tcolorbox}
  true/false = nonconflict(DES1,DES2) tests whether DES1, DES2 are nonconflicting, namely whether all reachable states of the product DES are coreachable. Not for use with vocalized DES. Note that by doing the product of two DESs, only shared events are executed if their alphabets are not the same.
\end{tcolorbox}

\paragraph{Answer}:
%22

\textbf{Conflicting Languages:}

\begin{itemize}
  \item Language $L_1$: {0, 00, 000, ...} (The set of all strings consisting of '0' with different lengths, including the empty string $\epsilon$).
  \item Language $L_2$: {1, 11, 111, ...} (The set of all strings consisting of '1' with different lengths, including the empty string $\epsilon$).
\end{itemize}

In this case, the prefix-closure of $L_1$ is $L_1$, and the prefix-closure of $L_2$ is $L_2$. The intersection of their complements ($\overline{L_1} \cap \overline{L_2}$) is still $L_1$ and $L_2$, respectively. However, the intersection of the original languages ($\overline{L_1 \cap L_2}$) is the empty set ($\phi $), indicating that they are conflicting languages. There are no strings that belong to both $L_1$ and $L_2$ in their original forms.

\textbf{Non-Conflicting Languages:}

\begin{itemize}
  \item Language $A$: {0, 01, 001, 0001, ...} (The set of strings that start with '0' and can have any number of additional '0's or '1's).
  \item Language $B$: {1, 10, 110, 1110, ...} (The set of strings that start with '1' and can have any number of additional '0's or '1's).
\end{itemize}

In this case, the prefix-closure of $A$ is $A$, and the prefix-closure of $B$ is $B$. The intersection of their complements ($\overline{A} \cap \overline{B}$) is still $A$ and $B$, respectively. The intersection of the original languages ($\overline{A \cap B}$) is also non-empty and contains strings like "0" and "1" that belong to both $A$ and $B$. Therefore, $A$ and $B$ are non-conflicting languages.

To verify these results using TCT (Theory of Computation Tools), construct deterministic finite automata (DFAs) for the languages and then compare the intersections to determine whether they are conflicting or non-conflicting.

\section{Given an alphabet $\Sigma$, a language $L \subseteq  \Sigma^*$ is said to be prefix-closed (closed for simplicity) if $L = \overline{L}$. A language $L \subseteq  K$ is said to be closed with respect to K, or simply K-closed if $L = \overline{L} \cap K$.}

\begin{enumerate}[label=(\alph*)]
  \item Show that if two languages $L_1$ and $L_2$ are K-closed, then $L_1 \cap L_2$ is K-closed.
  \item Show that if $L = \overline{L}$ and define $L^\prime  = L \cap K$, then $L^\prime$ is K-closed.
  \item Propose a few examples of languages that are closed or K-closed given a language K.
\end{enumerate}

\paragraph{Answer}:
%23

\paragraph{(a)}:

\textbf{$L_1 \cap L_2$ is Prefix-Closed Within Itself:}

Since $L_1$ and $L_2$ are both K-closed, they are individually prefix-closed within themselves. This means that for any string in $L_1$ or $L_2$, it does not have any proper prefixes within $L_1$ or $L_2$.

Now, consider the intersection $L_1 \cap L_2$. For any string $\omega \in L_1 \cap L_2$, it means that $\omega$ is both in $L_1$ and $L_2$. Therefore, it follows that $\omega$ does not have any proper prefixes within $L_1$ (because $L_1$ is prefix-closed), and $\omega$ does not have any proper prefixes within $L_2$ (because $L_2$ is prefix-closed). As a result, $\omega$ does not have any proper prefixes within $L_1 \cap L_2$.

This demonstrates that $L_1 \cap L_2$ is prefix-closed within itself.

\textbf{$L_1 \cap L_2 \subset K$:}

Because $L_1$ and $L_2$ are both K-closed, it means that they are both subsets of K. That is, for any string in $L_1$ or $L_2$, it is also a member of K.

Now, let's consider the intersection $L_1 \cap L_2$. For any string $\sigma \in L_1 \cap L_2$, it means that $\omega$ is both in $L_1$ and $L_2$. Since $L_1$ and $L_2$ are both subsets of K, it follows that $\sigma$ is a member of K as well, as it satisfies the condition of being in both $L_1$ and $L_2$.

This demonstrates that $L_1 \cap L_2 \subset K$.

Therefore, we have shown that $L_1 \cap L_2$ is both prefix-closed within itself and a subset of K, which means that it is K-closed, as required.

\paragraph{(b)}:

we need to demonstrate that $L^\prime$ satisfies the properties of being prefix-closed within itself and being a subset of K.

\begin{enumerate}
  \item $L^\prime$ is Prefix-Closed Within Itself:
  
  Given that $L = \overline{L}$, it means that L is a prefix-closed language within itself. For any string $\omega \in L$, it does not have any proper prefixes within L.

  Now, consider the language $L^\prime$. For any string $\omega \in L^\prime$, it means that $\omega$ is both in L (which is prefix-closed) and in K. Since $\omega  \in L$, it does not have any proper prefixes within L. Additionally, because $\omega \in K$, it follows that $\omega$ is a member of K.
  
  Therefore, $\omega$ does not have any proper prefixes within $L^\prime$ (since it doesn't have any within L), and $\omega$ is a member of K.
  
  This demonstrates that $L^\prime$ is prefix-closed within itself.

  \item $L^\prime$ is a Subset of K:
  
  Given that $L^\prime = L \cap K$, it implies that $L^\prime$ is the intersection of L and K. Therefore, for any string $\omega \in L^\prime$, it means that $\omega$ is both in L and in K.

  Since $L = \overline{L}$, it follows that for any string in L (including those in $L^\prime$), they do not have any proper prefixes within L. Additionally, because $\omega \in K$, it follows that $\omega$ is a member of K.

  This demonstrates that $L^\prime$ is a subset of K.

  Therefore, we have shown that $L^\prime$ is both prefix-closed within itself and a subset of K, which means that it is K-closed, as required.

\end{enumerate}

\paragraph{(c)}:

\textbf{Closed Languages ($L = \overline{L}$):}

\begin{enumerate}
  \item Language $L_1: \{\epsilon\}$ (the language containing only the empty string)
  
  $L_1$ is closed because it is equal to its own complement. There are no proper prefixes of $\epsilon$, so it is prefix-closed.
  
  \item Language $L_2: \{0, 1, 00, 11, \ldots\}$ (the set of all strings consisting of '0' and '1', including the empty string $\epsilon$)
  
  $L_2$ is closed because it contains all possible strings of '0' and '1', and there are no proper prefixes within itself.
\end{enumerate}

\textbf{K-Closed Languages ($L = \overline{L} \cap K$):}

\begin{enumerate}
  \item Language $A: \{0, 00, 000, \ldots\}$ (the set of all strings consisting of '0' with different lengths, including the empty string $\epsilon$)
  
  If $K = \{0\}$, then A is K-closed because it is the intersection of its own complement and K. A consists of strings without proper prefixes, and it is a subset of K.
  
  \item Language $B: \{1, 11, 111, \ldots\}$ (the set of all strings consisting of '1' with different lengths, including the empty string $\epsilon$)
  
  If $K = \{1\}$, then B is K-closed because it is the intersection of its own complement and K. B consists of strings without proper prefixes, and it is a subset of K.

  \item Language $C: \{\epsilon, 00, 11, 0000, 1111, \ldots\}$ (the set of all strings consisting of repeated '0's or '1's, including the empty string $\epsilon$)
  
  If $K = \{\epsilon, 0, 1\}$, then C is K-closed because it is the intersection of its own complement and K. C consists of strings without proper prefixes, and it is a subset of K.

  \item Language $D: \{ab, aab, aaab, \ldots\}$ (the set of strings with 'a's followed by one or more 'b's)
  
  If $K = \{ab, aab, \ldots\}$, then D is K-closed because it is the intersection of its own complement and K. D consists of strings without proper prefixes, and it is a subset of K.

\end{enumerate}

\section{$\Delta $ One refers to a review article on the history of supervisory control theory (SCT) of DES if she/he is interested in the development of DES modeling, }

analysis and control, see [W. M. Wonham, K. Cai, and K. Rudie, Supervisory control of discrete event systems: A brief history, Annual Annual Reviews in Control, vol. 45, 2018, Pages 250-256]. A similar version can be found in \url{https://www.control.utoronto.ca/~wonham/Wonham_SCDES_history.pdf}

\paragraph{Answer}:

%24

\section{* (Petri nets and discrete event systems) Petri nets serve as an important yet powerful alternative to automata for the modeling and control of untimed DES.}

\paragraph{Answer}:

%25

\section{** Conventionally, people think of models as either toys or simple copies of reality.}

\paragraph{Answer}:

%26

\section{A computer system operates with two parallel processors P1 and P2. The total capacity (queue and server included) of P1 is $K_1 = 1$, and that of P2 is $K_2 = 2$. The system receives two types of jobs, labeled $J_1$ and $J_2$. Jobs of type $J_1$ must be processed at P1, and jobs of type $J_2$ must be processed at P2. When a job is processed, it leaves the system. If a job finds a full queue upon arrival, then the job is simply rejected. Build an automaton model of this system.}

\begin{tcolorbox}
  Four events are identified: 
  
  $a_i$: arrival of $J_i$ 

  $d_i$: departure of $J_i$.
  
  A state is of the form ($n_1$, $n_2$) where $n_i$ is the number of jobs in (queue and server) in processor $P_i$. 
  
  Note that the model has six states totally.
\end{tcolorbox}

\paragraph{Answer}:

%27

States:

\begin{itemize}
  \item State (0, 0): Initial state. No jobs are in the system.
  \item State (1, 0): A job of type $J_1$ is in the queue for P1.
  \item State (2, 0): Two jobs of type J1 are in the queue for P1 (queue full, no room for $J_2$).
  \item State (0, 1): A job of type $J_2$ is in the queue for P2.
  \item State (0, 2): Two jobs of type $J_2$ are in the queue for P2 (queue full, no room for $J_1$).
  \item State (1, 1): A job of type $J_1$ is in the queue for P1, and a job of type $J_2$ is in the queue for P2 (both queues occupied).
\end{itemize}

Transitions:

\begin{enumerate}
  \item From State (0, 0):
  \begin{itemize}
    \item On arrival of a job of type $J_1$, transition to State (1, 0).
    \item On arrival of a job of type $J_2$, transition to State (0, 1).
  \end{itemize}
  \item From State (1, 0):
  \begin{itemize}
    \item On arrival of another job of type $J_1$, transition to State (2, 0) (queue for P1 is full).
    \item On arrival of a job of type $J_2$, transition to State (1, 1) ($J_2$ joins P2's queue).
    \item On departure of the $J_1$ job from P1, transition back to State (0, 0).
  \end{itemize}
  \item From State (2, 0):
  \begin{itemize}
    \item On arrival of a job of type $J_1$, stay in State (2, 0) (queue is full, job rejected).
    \item On arrival of a job of type $J_2$, stay in State (2, 0) (queue is full, job rejected).
    \item On departure of the $J_1$ job from P1, transition to State (0, 0) (queue for P1 is free).
  \end{itemize}
  \item From State (0, 1):
  \begin{itemize}
    \item On arrival of a job of type $J_1$, transition to State (1, 1) ($J_1$ joins P1's queue).
    \item On arrival of another job of type $J_2$, transition to State (0, 2) (queue for P2 is full).
    \item On departure of the $J_2$ job from P2, transition back to State (0, 0).
  \end{itemize}
  \item From State (0, 2):
  \begin{itemize}
    \item On arrival of a job of type $J_1$, stay in State (0, 2) (queue is full, job rejected).
    \item On arrival of a job of type $J_2$, stay in State (0, 2) (queue is full, job rejected).
    \item On departure of the $J_2$ job from P2, transition to State (0, 1) (queue for P2 is free).
  \end{itemize}
  \item From State (1, 1):
  \begin{itemize}
    \item On arrival of another job of type $J_1$, stay in State (1, 1) (both queues are full, job rejected).
    \item On arrival of another job of type $J_2$, stay in State (1, 1) (both queues are full, job rejected).
    \item On departure of the $J_1$ job from P1, transition to State (0, 1) (queue for P1 is free).
    \item On departure of the $J_2$ job from P2, transition to State (1, 0) (queue for P2 is free).
  \end{itemize}
  
\end{enumerate}


\section{A workcell consists of two machines M1 and M2 and an automated guided vehicle AGV, along with two auxiliary devices: input buffer and output buffer whose capacity is assumed to be large enough.}

\paragraph{Answer}:

%28

\section{Propose the Petri net model for the system in Question 28.}

\paragraph{Answer}:

%29

\section{Given $L, L_1, L_2, L_3 \subseteq  \Sigma^*$, show}

\begin{gather*}
  L \subseteq \overline{L}, \\
  \overline{ L_1 \cap L_2 } \subseteq \overline{L_1} \cap \overline{L_2}, \\
  L_1(L_2 \cup L_3) = L_1L_2 \cup L_1L_3, and. \\
  L_1(L_2 \cap L_3) = L_1L_2 \cap L_1L_3.
\end{gather*}

\paragraph{Answer}:

%30

\begin{itemize}
  \item $L \subseteq \overline{L}$
  
  To show that a language $L$ is a subset of its complement, you want to prove that for every string $x$ in $L$, it is also in $\overline{L}$, which means that $L \subseteq \overline{L}$.

  Here's the formal proof:

  Let $x$ be an arbitrary string in $L$. This means that $x$ is an element of $L$. Since $x$ is in $L$, it is also not in $\overline{L}$, by definition. Therefore, $x$ is in $L$ and not in $\overline{L}$.

  Because $x$ was an arbitrary string chosen from $L$, this proof holds for all strings in $L$. Therefore, we can conclude that for every string $x$ in $L, x$ is also in $\overline{L}$, which means $L \subseteq \overline{L}$.

  In other words, the entire language $L$ is a subset of its own complement $\overline{L}$.

  \item $\overline{ L_1 \cap L_2 } \subseteq \overline{L_1} \cap \overline{L_2}$
  
  To show that $\overline{L_1 \cap L_2} \subseteq \overline{L_1} \cap \overline{L_2}$, we need to prove that if a string is in the complement of the intersection of two languages $L_1$ and $L_2$, then it is also in the intersection of the complements of these languages.

  Let's use set notation to represent this:
  
  $\overline{L_1 \cap L_2}$ represents the complement of the intersection of $L_1$ and $L_2$.
  
  $\overline{L_1} \cap \overline{L_2}$ represents the intersection of the complements of $L_1$ and $L_2$.
  
  We want to prove that for any string $x$:
  
  If $x \in \overline{L_1 \cap L_2}$, then $x \in \overline{L_1} \cap \overline{L_2}$.
  
  Here's the proof:
  
  Suppose $x \in \overline{L_1 \cap L_2}$. This means that $x$ is not in the intersection of $L_1$ and $L_2$. In other words, $x \notin (L_1 \cap L_2)$.
  
  Now, we can use De Morgan's Law, which states that the complement of an intersection is equal to the union of complements:
  
  $\overline{L_1 \cap L_2} = \overline{L_1} \cup \overline{L_2}$
  
  So, we have $x \in (\overline{L_1} \cup \overline{L_2})$. This means that $x$ is in either the complement of $L_1$ or the complement of $L_2$, or both.
  
  Therefore, $x \in \overline{L_1}$ and $x \in \overline{L_2}$.
  
  This is precisely what is represented by $x \in \overline{L_1} \cap \overline{L_2}$, which is the intersection of the complements of $L_1$ and $L_2$.
  
  So, we have shown that if $x \in \overline{L_1 \cap L_2}$, then $x \in \overline{L_1} \cap \overline{L_2}$, which proves the inclusion $\overline{L_1 \cap L_2} \subseteq \overline{L_1} \cap \overline{L_2}$.

  \item $L_1(L_2 \cup L_3) = L_1L_2 \cup L_1L_3$
  
  To prove that $L_1(L_2 \cup L_3) = L_1L_2 \cup L_1L_3$, we need to show that every string in the left-hand side (LHS) language is also in the right-hand side (RHS) language, and vice versa.

  Let's break this proof into two parts:
  
  Part 1: Proving $L_1(L_2 \cup L_3) \subseteq L_1L_2 \cup L_1L_3$:
  
  Let $x$ be a string in $L_1(L_2 \cup L_3)$. This means that $x$ is of the form $x = yz$, where $y \in L_1$ and $z \in (L_2 \cup L_3)$. Since $z \in (L_2 \cup L_3)$, it means that $z$ can be either in $L_2$ or $L_3$.
  
  \begin{enumerate}
    \item If $z \in L_2$, then $yz$ is in $L_1L_2$, and therefore, $x$ is in $L_1L_2 \cup L_1L_3$.
    
    \item If $z \in L_3$, then $yz$ is in $L_1L_3$, and therefore, $x$ is in $L_1L_2 \cup L_1L_3$.
  \end{enumerate}
  
  In both cases, $x$ is in $L_1L_2 \cup L_1L_3$. Therefore, we have shown that $L_1(L_2 \cup L_3) \subseteq L_1L_2 \cup L_1L_3$.
  
  Part 2: Proving $L_1(L_2 \cup L_3) \supseteq L_1L_2 \cup L_1L_3$:
  
  Let $x$ be a string in $L_1L_2 \cup L_1L_3$. This means that $x$ is either in $L_1L_2$ or $L_1L_3$.
  
  \begin{enumerate}
    \item If $x$ is in $L_1L_2$, it means $x = yz$, where $y \in L_1$ and $z \in L_2$. Since $z \in L_2$, we can say that $z$ is in $(L_2 \cup L_3)$, and therefore, $x$ is in $L_1(L_2 \cup L_3)$.
    \item If $x$ is in $L_1L_3$, it means $x = yz$, where $y \in L_1$ and $z \in L_3$. Similar to the first case, since $z \in L_3$, we can say that $z$ is in $(L_2 \cup L_3)$, and therefore, $x$ is in $L_1(L_2 \cup L_3)$.
  \end{enumerate}
  
  In both cases, $x$ is in $L_1(L_2 \cup L_3)$. Therefore, we have shown that $L_1(L_2 \cup L_3) \supseteq L_1L_2 \cup L_1L_3$.
  
  Combining both parts, we have proven that $L_1(L_2 \cup L_3) = L_1L_2 \cup L_1L_3$.

  \item $L_1(L_2 \cap L_3) = L_1L_2 \cap L_1L_3$
  
  To prove that $L_1(L_2 \cap L_3) = L_1L_2 \cap L_1L_3$, we need to show that every string in the left-hand side (LHS) language is also in the right-hand side (RHS) language, and vice versa.

  Let's break this proof into two parts:

  Part 1: Proving $L_1(L_2 \cap L_3) \subseteq L_1L_2 \cap L_1L_3$:

  Let $x$ be a string in $L_1(L_2 \cap L_3)$. This means that $x$ is of the form $x = yz$, where $y \in L_1$ and $z \in (L_2 \cap L_3)$. Since $z \in (L_2 \cap L_3)$, it means that $z$ is in both $L_2$ and $L_3$.

  \begin{enumerate}
    \item Since $y \in L_1$ and $z \in L_2$, $yz$ is in $L_1L_2$, and therefore, $x$ is in $L_1L_2 \cap L_1L_3$.
    \item Since $y \in L_1$ and $z \in L_3$, $yz$ is in $L_1L_3$, and therefore, $x$ is in $L_1L_2 \cap L_1L_3$.
  \end{enumerate}
  

  In both cases, $x$ is in $L_1L_2 \cap L_1L_3$. Therefore, we have shown that $L_1(L_2 \cap L_3) \subseteq L_1L_2 \cap L_1L_3$.

  Part 2: Proving $L_1(L_2 \cap L_3) \supseteq L_1L_2 \cap L_1L_3$:

  Let $x$ be a string in $L_1L_2 \cap L_1L_3$. This means that $x$ is in both $L_1L_2$ and $L_1L_3$.

  \begin{enumerate}
    \item If $x$ is in $L_1L_2$, it means $x = yz$, where $y \in L_1$ and $z \in L_2$. Since $z \in L_2$, it follows that $z$ is in $L_2 \cap L_3$, and therefore, $x$ is in $L_1(L_2 \cap L_3)$.
    \item If $x$ is in $L_1L_3$, it means $x = yz$, where $y \in L_1$ and $z \in L_3$. Similar to the first case, $z$ is in $L_2 \cap L_3$, and therefore, $x$ is in $L_1(L_2 \cap L_3)$.
  \end{enumerate}
  
  In both cases, $x$ is in $L_1(L_2 \cap L_3)$. Therefore, we have shown that $L_1(L_2 \cap L_3) \supseteq L_1L_2 \cap L_1L_3$.

  Combining both parts, we have proven that $L_1(L_2 \cap L_3) = L_1L_2 \cap L_1L_3$.

\end{itemize}




\section{Let G be a generator with the alphabet $\Sigma$ and $K \subseteq \Sigma^*$ be a language. Show $\overline{K} \subseteq L(G)$ if $K \subseteq L(G)$: [hint: L(G) is prefix-closed].}

\paragraph{Answer}:
%31
To prove that the complement of language $\overline{K} \subseteq L(G)$ when $K \subseteq of L(G)$, given that L(G) is prefix-closed, we can use the property of prefix-closed languages.

\textbf{Proof:}

Suppose $K \subseteq L(G)$. This means that for every string x in K, x is also in L(G) because $K \subseteq L(G)$.

Since L(G) is prefix-closed, we know that if a string x is in L(G), all of its prefixes are also in L(G).

Now, let's consider the complement of K, which is $\overline{K}$. For any string y in $\overline{K}$, it means that y is not in K (because $\overline{K}$ is the set of all strings not in K).

Since $K \subseteq L(G)$, if y is not in K, it is also not in L(G). This is because L(G) contains K, and if a string is not in K, it's not in L(G) either.

Therefore, for any string y in $\overline{K}$, it is not in K, and it is also not in L(G).

This implies that every string in $\overline{K}$ is also not in L(G), which is equivalent to saying that $\overline{K} \subseteq L(G)$.

In conclusion, if $K \subseteq L(G)$ and L(G) is prefix-closed, then the complement of K, $\overline{K} \subseteq L(G)$.

\section{Given a generator G with $\Sigma = \{a, b, c\}$ as shown below, assume that b is not observable. Find P (G).}

\paragraph{Answer}:
%32
To determine the observable part (P(G)) of a generator G with an alphabet $\Sigma$, you need to find the set of symbols in $\Sigma$ that are observable, which means they appear in the generated strings. In this case, we have $\Sigma = \{a, b, c\}$ and it is mentioned that b is not observable. So, we need to find the observable symbols, which are $\{a, c\}$, and that will be P(G).

So, in this case, $P(G) = \{a, c\}$, since b is not observable.

\section{Show the properties of projection.}

\begin{enumerate}
  \item If $A \subseteq B$, $P A \subseteq P B$ and $P^{-1}A \subseteq P^{-1}B$.
  \item $P [P^{-1}(L)] = L$; $L \subseteq P^{-1}[P (L)]$.
  \item $P (A \cup B) = P A \cup P B$.
  \item $P^{-1} (A \cup B) = P^{-1} (A) \cup P^{-1} (B)$; $P^{-1} (A \cap B) = P^{-1} (A) \cap P^{-1} (B)$.
  \item $P (AB) = P (A)P (B); P^{-1} (AB) = P^{-1} (A)P^{-1}(B)$.
\end{enumerate}

\paragraph{Answer}:
%33

\section{Given H, K, L, and $\Sigma = \Sigma_c \dot{\cup} \Sigma_u$ such that $H \subseteq K = K \subseteq L = L \subseteq \Sigma^*$, suppose that H is controllable with respect to K and $\Sigma_u$ and K is controllable with respect to L and $\Sigma_u$. Show that H is controllable with respect to L and $\Sigma_u$ (check whether $L = \overline{L}$ is a necessity for the controllability of H with respect to L and $\Sigma_u$).}

\paragraph{Answer}:
%34

\section{Let MACH as depicted below be a generator with $\Sigma = \{\alpha , \beta , \lambda , \mu \}$, where $\alpha$ and $\mu$ are controllable. Suppose that the SPEC is $K = \{\alpha\beta\alpha\beta\}$, i.e., MACH is shut down after two successful production cycles. By intuition, check if there exists a supervisor that can supervise MACH to implement this SPEC. If existing, portray it; otherwise, explain the reason of non-existence. Consider the case that $K = \{\alpha\beta\}$.}

\paragraph{Answer}:
%35

\end{document}