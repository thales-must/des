\documentclass{article}
\usepackage[top=1cm,left=1cm,right=1.5cm,bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\title{Discrete Event System}
\author{Tai Jiang}
\date{October 2023}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\begin{document}
  \maketitle
  \tableofcontents
  \pagenumbering{gobble}
  \newpage
  \pagenumbering{arabic}
\paragraph{Nomenclature}:

\begin{tabular}{l l}
  $\mathbb{N}$ & $\{0, 1, 2, \cdots  \} (set of natural numbers) $ \\
  $\mathbb{N}+$ & $\{1, 2, \cdots \} (set of positive integers)$ \\
  $\mathbb{N}_k$ & $\{0, 1, 2, \cdots , k\} (set of natural numbers from 0 up to k)$ \\
  $[a, b]$ & $\{a, a + 1, \cdots , b - 1, b\} \subseteq  N (a < b)$ \\
  $\mathbb{Z}$ & $\{\cdots , -2, -1, 0, 1, 2, \cdots \} (set of integers)$ \\
  $\mathbb{Q}$ & $\{a/b | a, b \in  Z, b \neq  0\} (set of rational numbers)$ \\
  $\mathbb{R}$ & set of real numbers \\
  $\mathbb{R}$ & $\geq 0 set of non-negative real numbers$ \\
  $\mathbb{R}+$ & set of positive real numbers \\
  $\mathbb{C}$ & set of complex numbers \\
\end{tabular}



\begin{tcolorbox}
  Remark: Editing the homework using LATEX is strongly preferred (Tex studio, a popular yet free software package (\url{https://www.texstudio.org/}), is recommended, where images with JPG, PNG, EPS, and PDF formats can be used). An alternative is overleaf which is an online package of LATEX tool, for details see \url{https://www.overleaf.com/learn}. A full tutorial for LATEX beginners is found in \url{https://www.youtube.com/watch?v=ydOTMQC7np0\&t=1830s}. Questions marked by $\star $ are optional (diﬀicult more or less), but more interesting. Those marked with double-star serve as hints for the related questions to be followed. The questions marked with $\Delta $ are (also optional) only for the students whose research interests fall into the DES area, which are much more heuristic and are expected to guide and channelize them to the cutting-edge topics by making practice on specific problems that serve for the starting point of their scientific research.
\end{tcolorbox}

\section{(Irrational number) Dedekind cut in mathematics is a concept advanced in 1872 by Richard Dedekind (1831-1916, German mathematician) that combines an arithmetic formulation of the idea of continuity with a rigorous distinction between rational and irrational numbers. }

Dedekind reasoned that the real numbers form an ordered continuum so that any two numbers x and y must satisfy one and only one of the conditions $x < y$, x = y, or $x > y$. He postulated a cut that separates the continuum into two subsets, say X and Y , such that if x is any member of X and y is any member of Y , then $x < y$. If the cut is made so that X has a largest rational member or Y a least member, then the cut corresponds to a rational number. If, however, the cut is made so that X has no largest rational member and Y no least rational member, then the cut corresponds to an irrational number.

For example, if X is the set of all real numbers x less than or equal to 22/7 and Y is the set of real numbers y greater than 22/7, then the largest member of X is the rational number 22/7. If, however, X is the set of all real numbers x such that $x^2$ is less than or equal to 2 and Y is the set of real numbers y such that $y^2$ is greater than 2, then X has no largest rational member and Y has no least rational member: the cut defines the irrational number: the square root of 2, i.e., $\sqrt{2}$.
\begin{tcolorbox}
  Question: Show that e is an irrational number (starting from e as an infinite series $e = 1+1+ \frac{1}{2!}  + \frac{1}{3!} +\ldots $).
\end{tcolorbox}


\paragraph{Answer:}
% q1
\begin{enumerate}
  \item Definition of Set A:
  \begin{itemize}
    \item A includes all rational numbers p such that $p < e$.
    \item This means A includes all such rational numbers as 1, 2, 2.5, 2.7, 2.71, ..., which are rational approximations to \textbf{e}.
  \end{itemize}
  \item Definition of Set B:
  \begin{itemize}
    \item B includes all rational numbers p such that $p > e$.
    \item This means B includes all such rational numbers as 3, 2.9, 2.8, 2.72, ..., which are rational approximations to \textbf{e}.
  \end{itemize}
\end{enumerate}

Now, we will prove that '\textbf{e}' is irrational, meaning it cannot be expressed as the ratio of two integers.

Assume that '\textbf{e}' is a rational number, $e = \frac{a}{b}$, where a and b are coprime integers (having a greatest common divisor of 1). Then, we can partition A and B into two subsets:

\begin{enumerate}
  \item $A' = \{p^* \in A: p < \frac{a}{b} \}$
  \item $B' = \{p^* \in B: p > \frac{a}{b} \}$
\end{enumerate}

Now, let $s = \frac{a}{b}$. Clearly, s belongs to both A' and B'. We can use the properties of Dedekind cuts to demonstrate that '\textbf{e}' is irrational.

For A', by Dedekind cut properties, there exists a maximum rational number $r \in A'$, such that $r < s$.

For B', also by Dedekind cut properties, there exists a minimum rational number $q \in B$, such that $q > s$.

Now, consider the rational numbers s and r. According to the construction, $r < s < q$.

However, by definition, A contains all rational numbers less than '\textbf{e}', and B contains all rational numbers greater than '\textbf{e}', so $r < e < q$.

This leads to a contradiction: $r < e < q$, where r and q are both rational numbers. This means that '\textbf{e}' cannot simultaneously belong to A' and B', contradicting the construction of Dedekind cuts.


\section{* Show that e (Euler constant, approximating 2.718281828...) is a transcendental number.}

\begin{tcolorbox}
  Generally speaking, a transcendental number is not algebraic in the sense that it is not the solution of an algebraic equation with rational-number coeﬀicients. Transcendental numbers are irrational, but not all irrational numbers are transcendental. For example, $x^2 - 2 = 0$ has the solutions $x = \sqrt{2}$; thus, the Square root of 2, an irrational number, is an algebraic number and not transcendental. Nearly all real and complex numbers are transcendental, but very few numbers have been proven to be transcendental. The numbers e and $ \pi $ are transcendental numbers. The Euler-Mascheroni constant $\gamma $
  
  \begin{equation*}
    \gamma = \lim_{n \to \infty}(- \log n+\sum_{k = 1}^{n} \frac{1}{k}  ) = 0.57721566490153286060651209008240243104215933593992\ldots      
  \end{equation*}
  
  has not proven to be transcendental but is generally believed to be by mathematicians.
\end{tcolorbox}

\begin{tcolorbox}
  Whether there is any transcendental number is not an easy question to answer. 
  The discovery of the first transcendental number by Joseph Liouville (1809-1882, French mathematician and engineer) in 1851 sparked up an interest in the field and began a new era in the theory of transcendental numbers. 
  In 1873, Charles Hermite (1822-1901, French mathematician) succeeded in proving that e is transcendental. And within a decade, Ferdinand von Lindemann (1852-1939, German mathematician) established the transcendence of $ \pi $ in 1882, which led to the impossibility of the ancient Greek problem of squaring the circle. 
  The theory has progressed significantly in recent years, with an answer to the Hilbert's seventh problem and the discovery of a nontrivial lower bound for linear forms of logarithms of algebraic numbers. 
  Although in 1874, the work of Georg Cantor (1845-1918, German mathematician) demonstrated the ubiquity of transcendental numbers (which is quite surprising), finding one or proving existing numbers are transcendental may be extremely hard. 
  For more details, see \url{https://en.wikipedia.org/wiki/Transcendental_number}.
\end{tcolorbox}

\paragraph{Answer}:
% q2
\begin{enumerate}
  \item Assume that 'e' is not transcendental and is algebraic (i.e., it is the root of a non-zero polynomial with integer coefficients).

  \item Consider the Taylor series expansion of 'e':
  
  $e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \frac{1}{4!} + \ldots$
  
  \item Now, suppose we have a polynomial P(x) with integer coefficients that has 'e' as a root.
  
  \item We can rewrite the Taylor series for 'e' as an infinite polynomial:
  
  $e = 1 + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \frac{1}{4!} + \ldots = 1 + \frac{x}{1!} + \frac{x^2}{2!}+ \frac{x^3}{3!} + \frac{x^4}{4!} + \ldots$
  
  \item We can compare the two polynomials: P(x) and the polynomial expansion of 'e'. If 'e' is a root of P(x), then P(e) = 0.
  
  \item Now, substitute 'e' into P(x) and expand it as a power series:
  
  $P(e) = a_0 + a_1e + a_2e^2 + a_3e^3 + \ldots$
  
  \item Since P(e) = 0, we have:
  
  $O = a_0 + a_1e + a_2e^2 + a_3e^3 + \ldots$
  
  \item By comparing coefficients of like terms on both sides of the equation, we obtain a power series that equals zero.
  
  \item However, this leads to a contradiction because 'e' is known to be transcendental, and it cannot be the root of any non-zero polynomial with integer coefficients.
  
  \item Therefore, the initial assumption that 'e' is algebraic must be false, which implies that 'e' is indeed transcendental.
\end{enumerate}

\section{* Get a rough picture of Naive Set Theory (via the lifetime of the great figures who contributed to set theory). There is a textbook \textit{Naive Set Theory} by Paul Halmos Originally published by Van Nostrand in 1960, reprinted in the Springer-Verlag Undergraduate Texts in Mathematics series in 1974. In this book, Halmos writes:}

\begin{tcolorbox}
  Every mathematician agrees that every mathematician must know some set theory; the disagreement begins in trying to decide how much is some. This book contains my answer ... with the minimum of philosophical discourse and logical formalism.
\end{tcolorbox}

\paragraph{Answer}:
% q3

Naive Set Theory is an elementary approach to set theory that deals with the basic concepts and principles of sets and functions without delving deeply into the more complex and formal aspects of axiomatic set theory. It provides a foundational understanding of sets, their properties, and their relationships, making it accessible to those with minimal background in mathematical logic and formalism. The development of set theory and the contributions of key figures in its history can be roughly summarized as follows:

\begin{enumerate}
  \item Georg Cantor (1845-1918): Cantor is often regarded as the founder of set theory. He introduced the concept of a set and developed the idea of different sizes of infinity, known as cardinal numbers. Cantor's work laid the foundation for many set theory concepts.
  \item Richard Dedekind (1831-1916): Dedekind made significant contributions to the development of set theory, including introducing the notion of Dedekind cuts for defining real numbers and the principle of mathematical induction.
  \item Ernst Zermelo (1871-1953) and Abraham Fraenkel (1891-1965): Zermelo-Fraenkel set theory, also known as ZFC, is a formal axiomatic system that provides a basis for modern set theory. Zermelo introduced the axioms of set theory, and Fraenkel later refined and extended them to form the ZFC set theory, which is widely used in mathematics today.
  \item Paul Halmos (1916-2006): Paul Halmos, a renowned mathematician, made significant contributions to various areas of mathematics, including set theory. His book "Naive Set Theory" was published in 1960 and has been influential in introducing students and mathematicians to the basics of set theory without delving into deep philosophical or formal aspects.
\end{enumerate}

Halmos's approach in \textbf{"Naive Set Theory"} is to provide an accessible introduction to sets, functions, and basic set-theoretic concepts without requiring extensive knowledge of formal logic. It focuses on intuitive understanding and practical applications in mathematics. While the book does not cover the most advanced aspects of set theory, it serves as a valuable resource for mathematicians and students who need a solid foundation in the subject.

Naive Set Theory is often used as a starting point for those looking to explore more advanced set theory and its various applications in mathematics, logic, and other fields.


\section{* Understand the development history of function (including injections, surjections, and bijections).}

\begin{tcolorbox}
  Historically, the concept of a function emerged in the 17th century as a result of the development of analytic geometry and the infinitesimal calculus, see the following material on the development of notion of function: \url{http://www.ms.uky.edu/~droyster/courses/fall06/PDFs/Chapter05.pdf}, \url{http://www. mr-ideahamster.com/classes/assets/a_evfcn.pdf}, \url{https://mathshistory.st-andrews.ac.uk/HistTopics/Functions/}, \url{https://www.researchgate.net/publication/251211596_The_history_ of_the_concept_of_function_and_some_educational_implications}.
\end{tcolorbox}

\paragraph{Answer}:
% q4

The concept of a function has a rich history that evolved over centuries. Here is a brief overview of the historical development of the notion of functions, including injections, surjections, and bijections:

\begin{enumerate}
  \item Ancient Roots: The idea of associating one quantity with another has ancient roots, with early mathematicians and scientists using functions informally. For example, ancient Greek mathematicians like Euclid and Diophantus worked with relationships between numbers, but they did not have a formal concept of a function.
  \item Analytic Geometry (17th Century): The concept of a function began to take shape in the 17th century with the development of analytic geometry by René Descartes. He introduced the coordinate plane, where geometric figures could be represented by equations. Functions were used to describe these equations and relationships between variables. However, the concept was still informal at this stage.
  \item Infinitesimal Calculus (17th-18th Century): The development of calculus by Isaac Newton and Gottfried Wilhelm Leibniz in the late 17th century further advanced the notion of functions. In calculus, functions were used to describe how one quantity (dependent variable) changes with respect to another (independent variable). The concept of limits, derivatives, and integrals played a crucial role in understanding functions more rigorously.
  \item Euler and Taylor (18th Century): Leonhard Euler and Brook Taylor made significant contributions to the study of functions and their expansions. Euler, in particular, worked with series expansions, which are integral to understanding functions more deeply.
  \item Cauchy and Rigor (19th Century): Augustin-Louis Cauchy and other mathematicians in the 19th century worked to provide a rigorous foundation for calculus, including the notion of functions. They introduced the epsilon-delta definition of limits, making functions a more formal concept.
  \item Dirichlet and Baire (19th-20th Century): Mathematicians like Peter Gustav Lejeune Dirichlet and René-Louis Baire made important contributions to the study of real and complex functions. They extended the concept of functions to more abstract spaces.
  \item Modern Set Theory (20th Century): The development of modern set theory, led by mathematicians like Ernst Zermelo and Abraham Fraenkel, provided a formal foundation for functions as set-theoretic objects. Functions were defined as sets of ordered pairs, with precise notions of injections, surjections, bijections, and function composition.
  \item Abstract Algebra (20th Century): In abstract algebra, the concept of functions was generalized in the form of group homomorphisms, ring homomorphisms, and other algebraic structures. These concepts extend the notion of functions beyond real and complex numbers.
  \item Category Theory (20th Century): Category theory provided a unifying framework for studying functions and morphisms across various mathematical structures. It introduced the concept of functors and natural transformations, allowing for a more abstract and generalized understanding of functions.
\end{enumerate}

Today, functions play a central role in nearly all branches of mathematics and have numerous applications in science and engineering. The historical development of the notion of functions reflects the evolving understanding of mathematical concepts and the increasing rigor applied to mathematical foundations.


\section{Euler's formula, named after Leonhard Euler (1707-1783, Swiss mathematician, physicist, astronomer, geographer, logician, and engineer), is a mathematical formula in complex analysis that establishes the fundamental relationship between the trigonometric functions and the complex exponential function. Euler's formula states that for any real number x:}

\begin{equation*}
  e^{ix} = cos x + i sin x,
\end{equation*}

where e is the base of the natural logarithm, i is the imaginary unit, and cos and sin are the trigonometric functions cosine and sine respectively. This complex exponential function is sometimes denoted cis x (cosine plus i sine). The formula is still valid if x is a complex number, and so some authors refer to the more general complex version as Euler's formula.

Eule's formula is ubiquitous in mathematics, physics, chemistry, and engineering. The physicist Richard Feynman (1918-1988, American theoretical physicist, received the Nobel Prize in Physics in 1965 jointly with Schwinger and Tomonaga) called the equation “our jewel” and “the most remarkable formula in mathematics”. When $x = \pi$, Euler's formula boils down to $e^{i\pi} + 1 = 0$ or $e^{i\pi} = -1$, which is known as Euler's identity.

\begin{tcolorbox}
  \textbf{Question:} Show (prove) Euler's formula using power-series expansions.
\end{tcolorbox}


\paragraph{Answer}:
% q5

Euler's formula, often written as \textbf{ "$e^{i \pi} + 1 = 0$"}, is a remarkable mathematical result that relates five of the most important constants in mathematics: e, i (the imaginary unit), $\pi$, 1, and 0. We can prove this formula using power series expansions and some properties of trigonometric functions. The key is to use the Maclaurin series (Taylor series centered at 0) for the exponential, sine, and cosine functions.

We know the Maclaurin series for the exponential function $e^x$ is:

$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!}  + \frac{x^4}{4!}  + \ldots $

Now, let's use this series for $x = i\pi$:

$e^{i\pi} = 1 + i\pi + \frac{(i\pi)^2}{2!} + \frac{(i\pi)^3}{3!} + \frac{(i\pi)^4}{4!}  + \ldots$

Simplify this expression:

$e^{i\pi} = 1 + i\pi - \frac{(\pi)^2}{2!}  - \frac{i\pi^3}{3!} + \frac{\pi^4}{4!}  + \ldots$

Now, let's consider the Maclaurin series for the sine and cosine functions:

\begin{equation*}
  \begin{aligned}
    \sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \ldots \\
    \cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \ldots  
  \end{aligned}
\end{equation*}

Using these series for $x = \pi$:

\begin{equation*}
  \begin{aligned}
    \sin(\pi) &= 0 \\
    \cos(\pi) &= -1
  \end{aligned}
\end{equation*}

Now, we can express $e^{i\pi}$ as a combination of sine and cosine:

\begin{equation*}
  \begin{aligned}
    e^{i\pi} &= 1 + i\pi - \frac{\pi^2}{2!}  - \frac{i\pi^3}{3!} + \frac{\pi^4}{4!}  + \ldots \\
    &= (1 - \frac{\pi^2}{2!} + \frac{\pi^4}{4!} - \ldots)  + i(\pi - \frac{\pi^3}{3!} + \frac{\pi^5}{5!} - \ldots)
  \end{aligned}
\end{equation*}

Now, notice that the real part is the Maclaurin series for the cosine of $\pi$, and the imaginary part is the Maclaurin series for the sine of $\pi$. We already established that $\cos(\pi) = -1$ and $\sin(\pi) = 0$, so we have:

\begin{equation*}
  \begin{aligned}
    e^{i\pi} &= -i + 0i \\
    e^{i\pi} &= -1    
  \end{aligned}
\end{equation*}

Now, we can rearrange Euler's formula:

$e^{i\pi} + 1 = -1 + 1 = 0$

So, we've shown that Euler's formula, $e^{i\pi} + 1 = 0$, is indeed true using power series expansions and trigonometric properties.

\section{Fermat's last theorem (proposed by \href{https://en.wikipedia.org/wiki/Pierre_de_Fermat}{Pierre Fermat} (1607-1665, lawyer and government oﬀicial in Toulouse, France, who did mathematics on the side for fun) around 1637) states that for all $x, y, z, n \in \mathbb{N} $ with $n \leq 3$, there is no solution to}

\begin{equation*}
  x^n + y^n = z^n
\end{equation*}

A proof was done by British mathematician \href{https://en.wikipedia.org/wiki/Andrew_Wiles}{Andrew Wiles} (1953-) in 1996 with hundreds of pages. He actually proved the Shimura-Taniyama-Weil Conjecture that is related to modular forms and elliptic curves which are very complicated and abstract notions in mathematics. The following proof is from a Russian blog. It is interesting to identify and discern the errors in the proof.

As known, when n is equal to 2, it gives us an infinite family of solutions called the Pythagorean triples such as (3,4,5), (6,8,10), (12,5,13), etc. To start with, by contradiction, we suppose that we have natural numbers x, y, z, and $n \leq 3$ such that $x^n + y^n = z^n$.

Define a new number $r \in \mathbb{R} $ that is a real number such that

\begin{equation*}
  x^2 + y^2 = r^2
\end{equation*}

This decides a triangle as shown below with AC = x, AB = y, and the hypotenuse BC = r. By $n \leq 3$, $x^n > x^2$, and $y^n > y^2$, we have

\begin{equation*}
  r^n = (r^2)^{n/2} = (x^2 + y^2)^{n/2} > x^n + y^n = z^n
\end{equation*}

This means that r is bigger than z. We shorten the hypotenuse r while it will no longer be a hypotenuse because if we shorten this side, the right angle will decrease. By leaving y and x the same length and shortening r until it coincides with the number z, we get the figure on the right-hand side.

Now, the angle $\angle BAC$ (with a side of length z) is not a right angle and it is actually an acute angle, denoted by $\Theta$ with $\Theta \in [0, \frac{\pi}{2} ]$. The Law of Cosines (also called the Cosine Rule) says

\begin{equation*}
  z^2 = x^2 + y^2 - 2xy \cos \Theta
\end{equation*}

We have then

\begin{equation*}
  \cos \Theta = \frac{1}{2xy} (x^2 + y^2 - z^2)
\end{equation*}

The author offering this proof tells us that we find a contradiction as notice what we have built is a value of cosine which is rational (note that a number is said to be rational if it can be written as the form $\frac{a}{b} $ , where
$a, b \in Z$ and $b \neq 0$). Since the cosine function is continuous and $\Theta$ can be arbitrary, it is impossible that all cosine values are rational, leading to a contradiction. This completes the proof of Fermat’s last theorem. However, we know that the proof is definitely incorrect. Find the error in the proof.

\paragraph{Answer}:
wait
% 6

\section{Consider the motion of a rigid body with friction retarding its motion, which is proportional to the speed of the body with a proportionality constant, as shown in the free-body diagram that defines coordinates. It shows all forces acting on the body (heavy lines) and indicates the acceleration (dashed line). The coordinate of the body's position x is the distance from the reference line shown and is chosen so that the positive is to the right. Note that in this case, the inertial acceleration is simply the second derivative of x (i.e., $a = \ddot{x}$) because the body position is measured with respect to an inertial reference. Suppose that initially we have $x(0) = \dot{x}(0) = 0$.}

\begin{tcolorbox}
  The Laplace transform can be used in some cases to solve linear constant coeﬀicient differential equations with given initial conditions. For details, one refers to A. D. Polyanin, Handbook of Linear Partial Differential Equations for Engineers and Scientists, Chapman \& Hall/CRC Press, Boca Raton, 2002.
\end{tcolorbox}

The equation of motion is found using Eq. (1). The friction force acts opposite to the direction of motion; therefore it is drawn opposite the direction of positive motion and entered as a negative force in Eq. \eqref{eq:eq1}.

\begin{equation}
  m\ddot{x} = u - b\dot{x}
  \label{eq:eq1}
\end{equation}

Suppose that m=1000 Kg, b = 50N · sec/m, and u = 500N. Find the solution of x and $\dot{x}$ and draw (using MATLAB) the response of the body to the step input u for $\dot{x}$. Change the parameter b and find solutions again.

\paragraph{Answer}:
wait
% 7

\section{The problem of determining whether a propositional formula (i.e., a compound proposition) is a tautology is fundamental in propositional logic. If there are n variables occurring in a formula then there are $2^n$ distinct valuations for the formula. Therefore, the task of determining whether or not the formula is a tautology can be done in a brute-force way: one needs to evaluate the truth value of the formula under each of its possible valuations. Verify that the following is a tautology.}

\begin{itemize}
  \item $\lnot p \land (p \lor q) \to q$.
  \item $(p \land q) \to r \Leftrightarrow p \to (q \to r) $.
  \item $[(p \to q) \land (q \to r)] \to [p \to r]$
\end{itemize}

The proof of a tautology or a contradiction can be done by means of logic equivalence laws (a list of equivalence laws can be found in Chapter II). For example, $(p \land q) \to (p \lor q)$ is a tautology, i.e., $(p \land q) \to (p \lor q) \equiv \mathsf{T} $, which can be shown by the following logic equivalences:

\begin{equation*}
  \begin{aligned}
    (p \land q) \to (p \lor q) & \equiv \lnot (p \land q) \lor (p \lor q) \text{Substitution for} \to \\
    & \equiv (\lnot p \land \lnot q) \lor (p \lor q) \text{De Morgan} \\
    & \equiv (\lnot p \lor q) \lor (\lnot q \lor q) \text{Commutativity and Associativity} \\
    & \equiv \mathsf{T} \lor \mathsf{T} text{Becouse of} \lnot p \lor p \equiv \mathsf{T} \\
    & \equiv \mathsf{T}
  \end{aligned}
\end{equation*}

Try to verify the above tautologies using the logic equivalence laws.

\begin{tcolorbox}
  For logic equivalence laws in Wiki, see \url{https://en.wikipedia.org/wiki/Logical_equivalence}. In addition to many, there is a nice webpage with a video for logical equivalence with 13 examples, see \url{https://calcworkshop.com/logic/logical-equivalence/}.
\end{tcolorbox}

\paragraph{Answer}:
wait
% 8


\section{Logical implication is a type of relationship between two statements or sentences. Even for a single mathematical statement, there exists implicit logical implication among its variables (arguments). The relation translates verbally into “logically implies” or the logical connective “if/then” and is symbolized by a double-lined arrow pointing toward the right $\Longrightarrow $}

In logic, implication is the relationship between different propositions where the second proposition is a logical consequence of the first. For instance, if A and B represent semantic statements, then $A \Longrightarrow B$ means “A implies B” or “If A, then B.” The word “implies” is used in the strongest possible sense.

\begin{tcolorbox}
  \textbf{Question:} Propose a logical implication formula for the statement $x \leq y$ (suppose that x and y are real numbers). Moreover, from this example, we will explore the equivalence of the two logical formulas $P \lor Q and \lnot P \Longrightarrow Q$ by a gut feeling.
\end{tcolorbox}

\paragraph{Answer}:
% 9

The logical implication formula for the statement \textbf{"$x \leq y$"} is:

$x \leq y \Rightarrow True$

This formula states that if "x is less than or equal to y," then the implication is True. In other words, if the condition  \textbf{"$x \leq y$"} is met, then the implication is always True, indicating that the statement \textbf{"$x \leq y$"} is satisfied.

Now, let's explore the equivalence of the two logical formulas:

\begin{equation*}
  \begin{aligned}
    P \lor Q \\
    \lnot P \Rightarrow Q
  \end{aligned}  
\end{equation*}

The formula $P \lor Q$ represents a logical "or" statement, meaning it is True if either P or Q is True (or both). In other words, it allows for multiple possibilities, and it's True as long as at least one of the conditions P or Q is satisfied.

The formula $\lnot P \Rightarrow Q$ represents a logical implication. It states that if P is not True ($\not P$) then Q must be True. In this case, it's a conditional statement, and it implies that if the condition P is not met, then the condition Q must be met.

These two formulas are not equivalent. The key difference is that in the first formula ($P \lor Q$), you have the flexibility that either P or Q (or both) can be True to make the statement True. In the second formula ($\lnot P \longrightarrow Q$), it specifically states that if P is not True, then Q must be True for the implication to be satisfied.

For example, let's use P to represent "It is raining" and Q to represent "I carry an umbrella."

\begin{itemize}
  \item The formula $P \lor Q$ means that if it's either raining or I carry an umbrella, the statement is True. This allows for the possibility that I might carry an umbrella even when it's not raining.
  \item The formula $\lnot P \Rightarrow Q$ means that if it's not raining, then I must carry an umbrella. This is a stronger condition, as it implies that the only situation where I would carry an umbrella is when it's not raining.

\end{itemize}

So, while these two logical formulas are related, they have different meanings and do not represent the same concept.


\section{Use predicate logic to express Goldbach's weak conjecture and Chen's theorem}

\paragraph{Answer}:
wait
%10

\section{Given an alphabet}

\paragraph{Answer}:
wait
%11

\section{When quantifiers in the same predicate are of the same quantity}

\paragraph{Answer}:
wait
%12

\section{Suppose that an alphabet}

\paragraph{Answer}:
wait
%13

\section{* The set of polynomials with integer coeﬀicients is countable.}

\paragraph{Answer}:
%14

Here's a more detailed explanation of the argument:

\begin{itemize}
  \item Each polynomial with integer coefficients can be represented as a finite sequence of its coefficients. For example, the polynomial:

  $P(x) = 3x^2 - 2x + 1$
  
  can be represented as the sequence of coefficients: [3, -2, 1].
  
  \item Since each coefficient is an integer, each element of the sequence is an element of the countable set of integers.
  
  \item The set of all finite sequences of integers is countable because it can be put into one-to-one correspondence with the set of natural numbers (positive integers). This can be done by considering sequences of length 1, length 2, length 3, and so on, and ordering them in a systematic way.
  
  \item Therefore, since we can map each polynomial with integer coefficients to a finite sequence of integers, and the set of finite sequences of integers is countable, the set of polynomials with integer coefficients is also countable.
\end{itemize}

In summary, the set of polynomials with integer coefficients is countable because it can be put into a one-to-one correspondence with the countable set of all finite sequences of integers, which can be systematically ordered and enumerated.

\section{* A complex number x is said to be algebraic if there are integers $a_0, a_1, . . ., a_n$, not all zero, such that $a_nx^n + a_{n-1}x^{n-1} + ... + a_1x + a_0 = 0$. Prove that the set of all algebraic numbers is countable.}

\paragraph{Answer}:
%15

Proof:

\begin{itemize}
  \item Consider the set of all polynomials with integer coefficients. Each polynomial can be represented as:

  $P(x) = a_nx^n + a_{n-1}x^(n-1) + ... + a_1*x + a_0$
  
  where a0, a1, ..., an are integers, and not all of them are zero.
  
  \item For each polynomial P(x), let's consider the set of its roots (solutions) in the complex numbers. These roots are the algebraic numbers associated with this polynomial.
  
  \item Now, for each polynomial P(x), the set of its roots is a finite set of complex numbers. These roots satisfy the polynomial equation P(x) = 0.
  
  \item We can define a mapping that associates each polynomial P(x) to its set of roots. In other words, we can create a function f(P) that takes a polynomial as input and outputs the set of roots for that polynomial.
  
  \item The number of such polynomials with integer coefficients is countable because the set of all finite sequences of integers is countable (as explained in a previous response).
  
  \item For each polynomial, the set of its roots is finite, and the union of countably many finite sets is still countable.
  
  \item Therefore, the set of algebraic numbers, which is the union of the sets of roots for all possible polynomials with integer coefficients, is also countable.
\end{itemize}

In conclusion, the set of algebraic numbers is countable because it can be shown that there are countably many polynomials with integer coefficients, and the set of algebraic numbers is a countable union of the finite sets of roots associated with these polynomials.

\section{Let $\Sigma = \{0, 1\}$, $A = \{\omega \in \Sigma ^* | \omega \text{ has the equal number of 01}\}$, and $B = \{0^*1^*\} = {0^m1^n | m \geq 0, n \geq 0}$. Write the expression of $A \cap  B$.}

\paragraph{Answer}:
%16

To write the expression for the intersection ($A \cap B$) of the two languages A and B, you need to find the strings that belong to both A and B. In this case, A represents strings with an equal number of '0's and '1's, and B represents strings that are in the form of $0^m1^n$, where m and n can be any non-negative integers.

The intersection $A \cap B$ will contain strings that satisfy both conditions, i.e., strings that have an equal number of '0's and '1's and are in the form of $0^m1^n$. The only string that satisfies both conditions is the empty string $\epsilon$, as it has an equal number of '0's and '1's (zero each) and is also in the form $0^m1^n$ for m = 0 and n = 0.

So, the expression is:

$A \cap  B = \{\epsilon\}$

In this case, the intersection of languages A and B contains only the empty string.

\section{* (Mathematical proof)}

\paragraph{Answer}:
%17

\section{(Methods of proof) Typical mathematical proof methods include Direct proof, Proof by mathematical induction, Proof by contrapositive, Proof by contradiction, Proof by construction, Proof by exhaustion, Probabilistic proof, Combinatorial proof, Nonconstructive proof, Statistical proof in pure mathematics, and Computerassisted proof. Here we focus on the proofs by contradiction and by contrapositive.}

\paragraph{Answer}:
%18


\end{document}